{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segragate file 將主要多個序列的檔案分成多個檔案\n",
    "- Blast only allow limited size （blast有限制檔案的大小）\n",
    "- Thus seperate it to several files （因此要將大個檔案分成多個小檔案）\n",
    "##### 3 parameters （三個可調參數）\n",
    "- *filename = original filename （檔案名字）\n",
    "- *path = original filename path （檔案路徑）\n",
    "- *fastadir = fasta directory （分割後存入的資料夾路徑）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"LWW016_all.fas\" #\n",
    "path = \"data\\\\fasta_file\\\\LWW016_all.fas\" #\n",
    "fastadir = \"data\\\\fasta_file\\\\output\" #\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def read_fas_as_list(path):\n",
    "    pattern = r\"[>][^>]+\"\n",
    "    with open (path,\"r\") as f:\n",
    "        whole_data = f.read()\n",
    "        return re.findall(pattern,whole_data)\n",
    "\n",
    "def part_folder (num = 50000):\n",
    "    datalist = read_fas_as_list(path)\n",
    "    count = 0\n",
    "    item_index = 0\n",
    "    folder_num = 1\n",
    "    temptxt = \"\"\n",
    "\n",
    "    while item_index < len(datalist):\n",
    "        count += len(datalist[item_index])\n",
    "        if count < num:\n",
    "            temptxt += datalist[item_index]\n",
    "            item_index += 1\n",
    "        else:\n",
    "            newpath = fastadir + \"\\\\\" + filename[:-4] + \"_\" + str(folder_num) + \".fas\"\n",
    "            with open (newpath, \"w\") as f:\n",
    "                f.write(temptxt)\n",
    "            folder_num += 1\n",
    "            count = len(datalist[item_index])\n",
    "            temptxt = datalist[item_index]\n",
    "            item_index += 1\n",
    "\n",
    "    newpath = fastadir + \"\\\\\" + filename[:-4] + \"_\" + str(folder_num) + \".fas\"\n",
    "    with open (newpath, \"w\") as f:\n",
    "                f.write(temptxt)\n",
    "\n",
    "part_folder ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BlastN using python （執行blast）\n",
    "- output .xml filetype （輸出類型為xml）\n",
    "##### parameters 可調參數\n",
    "- *outxmldir = output xml directory （xml的輸出路徑）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip\n",
    "\n",
    "outxmldir = \"data\\\\blast_result\\\\output\"\n",
    "\n",
    "#Blastn function\n",
    "\n",
    "from Bio.Blast import NCBIWWW\n",
    "\n",
    "def Blast_fasta(path,outpath):\n",
    "\n",
    "    with open (path,\"r\") as obj:\n",
    "        fasta_file = obj.read()\n",
    "\n",
    "    blast_result = NCBIWWW.qblast(\"blastn\", \"nt\", fasta_file, megablast=True, hitlist_size=10)\n",
    "    out = blast_result.read()\n",
    "\n",
    "    with open(outpath, \"w\") as output_xml:\n",
    "        output_xml.write(out)\n",
    "    blast_result.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LWW016_all_100_out.xml existed\n",
      "LWW016_all_101_out.xml existed\n",
      "LWW016_all_102_out.xml existed\n",
      "LWW016_all_103_out.xml existed\n",
      "LWW016_all_104_out.xml existed\n",
      "LWW016_all_105_out.xml existed\n",
      "LWW016_all_106_out.xml existed\n",
      "LWW016_all_107_out.xml existed\n",
      "LWW016_all_108_out.xml existed\n",
      "LWW016_all_109_out.xml existed\n",
      "LWW016_all_10_out.xml existed\n",
      "LWW016_all_110_out.xml existed\n",
      "LWW016_all_111_out.xml existed\n",
      "LWW016_all_112_out.xml existed\n",
      "LWW016_all_113_out.xml existed\n",
      "LWW016_all_114_out.xml existed\n",
      "LWW016_all_115_out.xml existed\n",
      "LWW016_all_116_out.xml existed\n",
      "LWW016_all_117_out.xml existed\n",
      "LWW016_all_118_out.xml existed\n",
      "LWW016_all_119_out.xml existed\n",
      "LWW016_all_11_out.xml existed\n",
      "LWW016_all_120_out.xml existed\n",
      "LWW016_all_121_out.xml existed\n",
      "LWW016_all_122_out.xml existed\n",
      "LWW016_all_123_out.xml existed\n",
      "LWW016_all_124_out.xml existed\n",
      "LWW016_all_125_out.xml existed\n",
      "LWW016_all_126_out.xml existed\n",
      "LWW016_all_127_out.xml existed\n",
      "LWW016_all_128_out.xml existed\n",
      "LWW016_all_129_out.xml existed\n",
      "LWW016_all_12_out.xml existed\n",
      "LWW016_all_130_out.xml existed\n",
      "LWW016_all_131_out.xml existed\n",
      "LWW016_all_132_out.xml existed\n",
      "LWW016_all_133_out.xml existed\n",
      "LWW016_all_134_out.xml existed\n",
      "LWW016_all_135_out.xml existed\n",
      "LWW016_all_136_out.xml existed\n",
      "LWW016_all_137_out.xml existed\n",
      "LWW016_all_13_out.xml existed\n",
      "LWW016_all_14_out.xml existed\n",
      "LWW016_all_15_out.xml existed\n",
      "LWW016_all_16_out.xml existed\n",
      "LWW016_all_17_out.xml existed\n",
      "LWW016_all_18_out.xml existed\n",
      "LWW016_all_19_out.xml existed\n",
      "LWW016_all_20_out.xml existed\n",
      "LWW016_all_21_out.xml existed\n",
      "LWW016_all_22_out.xml existed\n",
      "LWW016_all_23_out.xml existed\n",
      "LWW016_all_24_out.xml existed\n",
      "LWW016_all_25_out.xml existed\n",
      "LWW016_all_26_out.xml existed\n",
      "LWW016_all_27_out.xml existed\n",
      "LWW016_all_28_out.xml existed\n",
      "LWW016_all_29_out.xml existed\n",
      "LWW016_all_2_out.xml existed\n",
      "LWW016_all_30_out.xml existed\n",
      "LWW016_all_31_out.xml existed\n",
      "LWW016_all_32_out.xml existed\n",
      "LWW016_all_33_out.xml existed\n",
      "LWW016_all_34_out.xml existed\n",
      "LWW016_all_35_out.xml existed\n",
      "LWW016_all_36_out.xml existed\n",
      "LWW016_all_37_out.xml existed\n",
      "LWW016_all_38_out.xml existed\n",
      "LWW016_all_39_out.xml existed\n",
      "LWW016_all_3_out.xml existed\n",
      "LWW016_all_40_out.xml existed\n",
      "LWW016_all_41_out.xml existed\n",
      "LWW016_all_42_out.xml existed\n",
      "LWW016_all_43_out.xml existed\n",
      "LWW016_all_44_out.xml existed\n",
      "LWW016_all_45_out.xml existed\n",
      "LWW016_all_46_out.xml existed\n",
      "LWW016_all_47_out.xml existed\n",
      "LWW016_all_48_out.xml existed\n",
      "LWW016_all_49_out.xml existed\n",
      "LWW016_all_4_out.xml existed\n",
      "LWW016_all_50_out.xml existed\n",
      "LWW016_all_51_out.xml existed\n",
      "LWW016_all_52_out.xml existed\n",
      "LWW016_all_53_out.xml existed\n",
      "LWW016_all_54_out.xml existed\n",
      "LWW016_all_55_out.xml existed\n",
      "LWW016_all_56_out.xml existed\n",
      "LWW016_all_57_out.xml existed\n",
      "LWW016_all_58_out.xml existed\n",
      "LWW016_all_59_out.xml existed\n",
      "LWW016_all_5_out.xml existed\n",
      "LWW016_all_60_out.xml existed\n",
      "LWW016_all_61_out.xml existed\n",
      "LWW016_all_62_out.xml existed\n",
      "LWW016_all_63_out.xml existed\n",
      "LWW016_all_64_out.xml existed\n",
      "LWW016_all_65_out.xml existed\n",
      "LWW016_all_66_out.xml existed\n",
      "LWW016_all_67_out.xml existed\n",
      "LWW016_all_68_out.xml existed\n",
      "LWW016_all_69_out.xml existed\n",
      "LWW016_all_6_out.xml existed\n",
      "LWW016_all_70_out.xml existed\n",
      "LWW016_all_71_out.xml existed\n",
      "LWW016_all_72_out.xml existed\n",
      "LWW016_all_73_out.xml existed\n",
      "LWW016_all_74_out.xml existed\n",
      "LWW016_all_75_out.xml existed\n",
      "LWW016_all_76_out.xml existed\n",
      "LWW016_all_77_out.xml existed\n",
      "LWW016_all_78_out.xml existed\n",
      "LWW016_all_79_out.xml existed\n",
      "LWW016_all_7_out.xml existed\n",
      "LWW016_all_80_out.xml existed\n",
      "LWW016_all_81_out.xml existed\n",
      "LWW016_all_82_out.xml existed\n",
      "LWW016_all_83_out.xml existed\n",
      "LWW016_all_84_out.xml existed\n",
      "LWW016_all_85_out.xml existed\n",
      "LWW016_all_86_out.xml existed\n",
      "LWW016_all_87_out.xml existed\n",
      "LWW016_all_88_out.xml existed\n",
      "LWW016_all_89_out.xml existed\n",
      "LWW016_all_8_out.xml existed\n",
      "LWW016_all_90_out.xml existed\n",
      "LWW016_all_91_out.xml existed\n",
      "LWW016_all_92_out.xml existed\n",
      "LWW016_all_93_out.xml existed\n",
      "LWW016_all_94_out.xml existed\n",
      "LWW016_all_95_out.xml existed\n",
      "LWW016_all_96_out.xml existed\n",
      "LWW016_all_97_out.xml existed\n",
      "LWW016_all_98_out.xml existed\n",
      "LWW016_all_99_out.xml existed\n",
      "LWW016_all_9_out.xml existed\n",
      "LWW016_all_1.fas Blastn success!\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "#Run Blastn with \n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "outfilelist = []\n",
    "outfile = os.listdir(outxmldir)\n",
    "pattern = r\"[_][0-9]+\"\n",
    "for item in outfile:\n",
    "    result = re.search(pattern,item)\n",
    "    if result:\n",
    "        outfilelist.append(item[result.start():result.end()])\n",
    "        print(item + \" existed\")\n",
    "\n",
    "templist = []\n",
    "failed = []\n",
    "\n",
    "filelist = os.listdir(fastadir)\n",
    "filelist = sorted(filelist,reverse=True)\n",
    "for item in filelist:\n",
    "    result = (re.search(pattern,item))\n",
    "    temp = item[result.start():result.end()]\n",
    "    if temp in outfilelist:\n",
    "        pass\n",
    "    else:\n",
    "        path = fastadir + \"\\\\\" + item\n",
    "        outpath = outxmldir + \"\\\\\" + item[:-4]+ \"_out.xml\"\n",
    "        try:\n",
    "            Blast_fasta(path,outpath)\n",
    "            print(item + \" Blastn success!\")\n",
    "            print(\"-\"*10)\n",
    "        except:\n",
    "            print(item + \" Blastn failed!\")\n",
    "            templist.append(item)\n",
    "            pass\n",
    "\n",
    "print(failed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read xml （讀取xml檔案資料）\n",
    "- parse blastn output xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read xml information\n",
    "\n",
    "result_dict = {}\n",
    "temp_query = \"\"\n",
    "count = 0\n",
    "\n",
    "import xml.etree.ElementTree as  ET\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "filelist = os.listdir(outxmldir)\n",
    "filelist = sorted(filelist, reverse=True)\n",
    "\n",
    "for i in filelist:\n",
    "    path = outxmldir + \"\\\\\" + i\n",
    "    for event, item in ET.iterparse(path,events=(\"start\", \"end\")):\n",
    "        if item.tag == 'Iteration_query-def' and event == \"start\":\n",
    "            temp_query = item.text\n",
    "        if item.tag == 'Hit' and event == \"start\":\n",
    "            try:\n",
    "                if item[0].text == '1':\n",
    "                    count+=1\n",
    "                    try:\n",
    "                        if item[2].text[1]==\".\":\n",
    "                            item[2].text = item[2].text.replace(\".\", \" \")\n",
    "                        pattern = \"[\\w]+ [\\w]+\"\n",
    "                        reres = re.search(pattern, item[2].text)\n",
    "                        result_dict[temp_query] = [item[2].text[reres.start():reres.end()].replace(\" \", \"_\")] #sci_name\n",
    "                        result_dict[temp_query].append(item[3].text) #accession\n",
    "                        result_dict[temp_query].append(f\"{(int(item[5][0][10].text)/int(item[5][0][13].text))*100}\") #identity \n",
    "                    except:\n",
    "                        pass\n",
    "            except:\n",
    "                count+=1\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new output file \n",
    "（創新的fasta檔案，將搜尋到的結果輸入fasta檔案）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read original all file and modify info\n",
    "\n",
    "import re\n",
    "\n",
    "def read_all_file(path):\n",
    "    with open (path, \"r\") as obj:\n",
    "        txt = obj.read()\n",
    "    return txt    \n",
    "\n",
    "txt = read_all_file(path)\n",
    "\n",
    "for item in result_dict:\n",
    "    pattern = item\n",
    "    try:\n",
    "        txt = re.sub(pattern, \\\n",
    "            result_dict[item][0] + \"_\" + \\\n",
    "            result_dict[item][1] + \"_\" + \\\n",
    "            #result_dict[item][2][0:5], txt)\n",
    "            result_dict[item][2][0:5] + \"_\" + item, txt)\n",
    "        \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write output to fasta\n",
    "outputfilename = \"all_output.fas\"\n",
    "\n",
    "with open (outputfilename,\"w\") as obj:\n",
    "    obj.write(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blastn download as json （不用）\n",
    "- parse json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>accesssion</th>\n",
       "      <th>identity</th>\n",
       "      <th>sciname</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>k127_147_1_av_267.0000_len_1070</td>\n",
       "      <td>KY072797</td>\n",
       "      <td>75.7</td>\n",
       "      <td>Parnassius choui</td>\n",
       "      <td>Parnassius choui mitochondrion, complete genome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LWW0016_60_av_134.47</td>\n",
       "      <td>KP715146</td>\n",
       "      <td>99.8</td>\n",
       "      <td>Colias erate</td>\n",
       "      <td>Colias erate mitochondrion, complete genome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>k127_179532__av_81.1152_len_995</td>\n",
       "      <td>NC_045249</td>\n",
       "      <td>53.3</td>\n",
       "      <td>Hasora badra</td>\n",
       "      <td>Hasora badra voucher SB20120510 mitochondrion,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             query accesssion identity           sciname  \\\n",
       "0  k127_147_1_av_267.0000_len_1070   KY072797     75.7  Parnassius choui   \n",
       "1             LWW0016_60_av_134.47   KP715146     99.8      Colias erate   \n",
       "2  k127_179532__av_81.1152_len_995  NC_045249     53.3      Hasora badra   \n",
       "\n",
       "                                               title  \n",
       "0    Parnassius choui mitochondrion, complete genome  \n",
       "1        Colias erate mitochondrion, complete genome  \n",
       "2  Hasora badra voucher SB20120510 mitochondrion,...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = \"data\\\\blast_result\\\\test_blastoutput.json\"\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "query = []\n",
    "accesssion = []\n",
    "identity = []\n",
    "sciname = []\n",
    "title = []\n",
    "\n",
    "\n",
    "with open(file_name) as obj:\n",
    "    getDatas = json.load(obj)\n",
    "\n",
    "for getData in getDatas:\n",
    "    for item in getDatas[getData]:\n",
    "        query.append((item['report']['results']['search']['query_title']))\n",
    "        accesssion.append(item['report']['results']['search']['hits'][0]['description'][0]['accession'])\n",
    "        sciname.append(item['report']['results']['search']['hits'][0]['description'][0]['sciname'])\n",
    "        title.append(item['report']['results']['search']['hits'][0]['description'][0]['title'])\n",
    "        identity.append(item['report']['results']['search']['hits'][0][\"hsps\"][0]['identity']/10)\n",
    "\n",
    "\n",
    "df = pd.DataFrame([query, accesssion, identity, sciname, title]).transpose()\n",
    "df.columns = ['query', 'accesssion', 'identity', 'sciname', 'title']\n",
    "df\n",
    "#df.to_csv(\"data\\\\blast_result\\\\output\\\\output.csv\",index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88bd4de4ad0fcb3b44b0075f4057ac2f9688329ee6fcc1e7f274caa50a4527c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
