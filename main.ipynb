{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segragate file\n",
    "- Blast only allow limited size\n",
    "- Thus seperate it to several files\n",
    "- *filename = original filename\n",
    "- *path = original filename path\n",
    "- *fastadir = fasta directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"LWW016_all.fas\"\n",
    "path = \"data\\\\fasta_file\\\\LWW016_all.fas\"\n",
    "fastadir = \"data\\\\fasta_file\\\\output\"\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def read_fas_as_list(path):\n",
    "    pattern = r\"[>][^>]+\"\n",
    "    with open (path,\"r\") as f:\n",
    "        whole_data = f.read()\n",
    "        return re.findall(pattern,whole_data)\n",
    "\n",
    "def part_folder (num = 50000):\n",
    "    datalist = read_fas_as_list(path)\n",
    "    count = 0\n",
    "    item_index = 0\n",
    "    folder_num = 1\n",
    "    temptxt = \"\"\n",
    "\n",
    "    while item_index < len(datalist):\n",
    "        count += len(datalist[item_index])\n",
    "        if count < num:\n",
    "            temptxt += datalist[item_index]\n",
    "            item_index += 1\n",
    "        else:\n",
    "            newpath = fastadir + \"\\\\\" + filename[:-4] + \"_\" + str(folder_num) + \".fas\"\n",
    "            with open (newpath, \"w\") as f:\n",
    "                f.write(temptxt)\n",
    "            folder_num += 1\n",
    "            count = len(datalist[item_index])\n",
    "            temptxt = datalist[item_index]\n",
    "            item_index += 1\n",
    "\n",
    "    newpath = fastadir + \"\\\\\" + filename[:-4] + \"_\" + str(folder_num) + \".fas\"\n",
    "    with open (newpath, \"w\") as f:\n",
    "                f.write(temptxt)\n",
    "\n",
    "part_folder ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BlastN using python\n",
    "- output .xml filetype\n",
    "- *outxmldir = output xml directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "outxmldir = \"data\\\\blast_result\\\\output\"\n",
    "\n",
    "#Blastn function\n",
    "\n",
    "from Bio.Blast import NCBIWWW\n",
    "\n",
    "def Blast_fasta(path,outpath):\n",
    "\n",
    "    with open (path) as obj:\n",
    "        fasta_file = obj.read()\n",
    "\n",
    "    blast_result = NCBIWWW.qblast(\"blastn\", \"nt\", fasta_file, megablast=True, hitlist_size=10)\n",
    "    out = blast_result.read()\n",
    "\n",
    "    with open(outpath, \"w\") as output_xml:\n",
    "        output_xml.write(out)\n",
    "    blast_result.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LWW016_all_100_out.xml existed\n",
      "LWW016_all_10_out.xml existed\n",
      "LWW016_all_1_out.xml existed\n"
     ]
    }
   ],
   "source": [
    "#Run Blastn with \n",
    "\n",
    "import re\n",
    "outfilelist = []\n",
    "outfile = os.listdir(outxmldir)\n",
    "pattern = r\"[_][0-9]+\"\n",
    "for item in outfile:\n",
    "    result = re.search(pattern,item)\n",
    "    if result:\n",
    "        outfilelist.append(item[result.start():result.end()])\n",
    "        print(item + \" existed\")\n",
    "\n",
    "templist = []\n",
    "import os\n",
    "filelist = os.listdir(fastadir)\n",
    "filelist = sorted(filelist,reverse=True)\n",
    "for item in filelist:\n",
    "    result = (re.search(pattern,item))\n",
    "    temp = item[result.start():result.end()]\n",
    "    if temp in outfilelist:\n",
    "        pass\n",
    "    else:\n",
    "        path = fastadir + \"\\\\\" + item\n",
    "        outpath = outxmldir + \"\\\\\" + item[:-4]+ \"_out.xml\"\n",
    "        try:\n",
    "            Blast_fasta(path,outpath)\n",
    "            print(item + \" Blastn success!\")\n",
    "            print(\"-\"*10)\n",
    "        except:\n",
    "            print(item + \" Blastn failed!\")\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read xml\n",
    "- parse blastn output xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read xml information\n",
    "\n",
    "result_dict = {}\n",
    "temp_query = \"\"\n",
    "count = 0\n",
    "\n",
    "import xml.etree.ElementTree as  ET\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "filelist = os.listdir(outxmldir)\n",
    "filelist = sorted(filelist, reverse=True)\n",
    "\n",
    "for i in filelist:\n",
    "    path = outxmldir + \"\\\\\" + i\n",
    "    for event, item in ET.iterparse(path,events=(\"start\", \"end\")):\n",
    "        if item.tag == 'Iteration_query-def' and event == \"start\":\n",
    "            temp_query = item.text\n",
    "        if item.tag == 'Hit' and event == \"start\":\n",
    "            try:\n",
    "                if item[0].text == '1':\n",
    "                    count+=1\n",
    "                    try:\n",
    "                        if item[2].text[1]==\".\":\n",
    "                            item[2].text = item[2].text.replace(\".\", \" \")\n",
    "                        pattern = \"[\\w]+ [\\w]+\"\n",
    "                        reres = re.search(pattern, item[2].text)\n",
    "                        result_dict[temp_query] = [item[2].text[reres.start():reres.end()].replace(\" \", \"_\")] #sci_name\n",
    "                        result_dict[temp_query].append(item[3].text) #accession\n",
    "                        result_dict[temp_query].append(f\"{(int(item[5][0][10].text)/int(item[5][0][13].text))*100}\") #identity \n",
    "                    except:\n",
    "                        pass\n",
    "            except:\n",
    "                count+=1\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new output file\n",
    "- *path = original file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read original all file and modify info\n",
    "\n",
    "path = \"data\\\\fasta_file\\\\LWW016_all.fas\"\n",
    "\n",
    "import re\n",
    "\n",
    "def read_all_file(path):\n",
    "    with open (path, \"r\") as obj:\n",
    "        txt = obj.read()\n",
    "    return txt    \n",
    "\n",
    "txt = read_all_file(path)\n",
    "\n",
    "for item in result_dict:\n",
    "    pattern = item\n",
    "    try:\n",
    "        txt = re.sub(pattern, \\\n",
    "            result_dict[item][0] + \"_\" + \\\n",
    "            result_dict[item][1] + \"_\" + \\\n",
    "            result_dict[item][2] + \"_\" + item, txt)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write output to fasta\n",
    "outputfilename = \"all_output.fas\"\n",
    "\n",
    "with open (outputfilename,\"w\") as obj:\n",
    "    obj.write(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blastn download as json\n",
    "- parse json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>accesssion</th>\n",
       "      <th>identity</th>\n",
       "      <th>sciname</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>k127_147_1_av_267.0000_len_1070</td>\n",
       "      <td>KY072797</td>\n",
       "      <td>75.7</td>\n",
       "      <td>Parnassius choui</td>\n",
       "      <td>Parnassius choui mitochondrion, complete genome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LWW0016_60_av_134.47</td>\n",
       "      <td>KP715146</td>\n",
       "      <td>99.8</td>\n",
       "      <td>Colias erate</td>\n",
       "      <td>Colias erate mitochondrion, complete genome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>k127_179532__av_81.1152_len_995</td>\n",
       "      <td>NC_045249</td>\n",
       "      <td>53.3</td>\n",
       "      <td>Hasora badra</td>\n",
       "      <td>Hasora badra voucher SB20120510 mitochondrion,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             query accesssion identity           sciname  \\\n",
       "0  k127_147_1_av_267.0000_len_1070   KY072797     75.7  Parnassius choui   \n",
       "1             LWW0016_60_av_134.47   KP715146     99.8      Colias erate   \n",
       "2  k127_179532__av_81.1152_len_995  NC_045249     53.3      Hasora badra   \n",
       "\n",
       "                                               title  \n",
       "0    Parnassius choui mitochondrion, complete genome  \n",
       "1        Colias erate mitochondrion, complete genome  \n",
       "2  Hasora badra voucher SB20120510 mitochondrion,...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = \"data\\\\blast_result\\\\test_blastoutput.json\"\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "query = []\n",
    "accesssion = []\n",
    "identity = []\n",
    "sciname = []\n",
    "title = []\n",
    "\n",
    "\n",
    "with open(file_name) as obj:\n",
    "    getDatas = json.load(obj)\n",
    "\n",
    "for getData in getDatas:\n",
    "    for item in getDatas[getData]:\n",
    "        query.append((item['report']['results']['search']['query_title']))\n",
    "        accesssion.append(item['report']['results']['search']['hits'][0]['description'][0]['accession'])\n",
    "        sciname.append(item['report']['results']['search']['hits'][0]['description'][0]['sciname'])\n",
    "        title.append(item['report']['results']['search']['hits'][0]['description'][0]['title'])\n",
    "        identity.append(item['report']['results']['search']['hits'][0][\"hsps\"][0]['identity']/10)\n",
    "\n",
    "\n",
    "df = pd.DataFrame([query, accesssion, identity, sciname, title]).transpose()\n",
    "df.columns = ['query', 'accesssion', 'identity', 'sciname', 'title']\n",
    "df\n",
    "#df.to_csv(\"data\\\\blast_result\\\\output\\\\output.csv\",index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2217992099559a26038b70e11f392c50e16e825ff225ac028cf0711693f69c89"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
